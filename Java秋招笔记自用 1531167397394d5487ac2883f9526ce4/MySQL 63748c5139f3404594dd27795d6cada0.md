# MySQL

# 基础

## MySQL逻辑架构

### 执行一条select语句会发生什么？

![mysql查询流程.webp](MySQL%2063748c5139f3404594dd27795d6cada0/mysql%25E6%259F%25A5%25E8%25AF%25A2%25E6%25B5%2581%25E7%25A8%258B.webp)

- 连接器：建立连接，管理连接、校验用户身份；
- 查询缓存：查询语句如果命中查询缓存则直接返回，否则继续往下执行。MySQL 8.0 已删除该模块；
- 解析 SQL，通过解析器对 SQL 查询语句进行词法分析、语法分析，然后构建语法树，方便后续模块读取表名、字段、语句类型；
- 执行 SQL：执行 SQL 共有三个阶段：
    - 预处理阶段：检查表或字段是否存在；将 `select *` 中的 `*` 符号扩展为表上的所有列。
    - 优化阶段：基于查询成本的考虑， 选择查询成本最小的执行计划；
    - 执行阶段：根据执行计划执行 SQL 查询语句，从存储引擎读取记录，返回给客户端；

## SQL语句

### 连接

将多个表按照列进行关联。

- 内连接：返回两个表中连接字段匹配的行，如果一个表中的行在另一个表中没有匹配的行，则不会出现在查询结果中。
- 外连接：
    - 左外连接：驱动表数据全部显示，被驱动表不匹配的不会显示。
    - 右外连接：被驱动表数据全部显示，驱动表不匹配的不会显示。
- 交叉连接：笛卡尔积，驱动表每一行与被驱动表每一行的组合。

### DML、DDL和DCL

- DML是数据操纵语言，如insert、update、delete、select等。
- DDL是数据库定义语言，如create、drop、truncate、alter等。
- DCL是数据库控制语言，如grant授权等。

### **Drop、Delete与Truncate的区别**

- Drop是DDL，从数据库中删除表，包括数据行和所有相关的对象如索引，不可回滚。
- Delete是DML，删除表中符合条件的行，表结构还在，可以回滚。
- Truncate是DDL，删除表中的所有数据，表结构还在，不可回滚。

### count(*)、count(1)与count(字段)

- count(*)和count(1)本质相同，InnoDB每返回一条记录，count值+1。效率比主键字段好，因为主键字段要读取值。
- count(1)、 count(*)、 count(主键字段)在执行的时候，如果表里存在二级索引，优化器就会选择二级索引进行扫描，因为二级索引占用更小的存储空间。
- count(字段)比count(主键字段)效率差，因为会全表扫描，所以建议给这个字段建立二级索引。

### DDL

```sql
# 数据库
CREATE DATABASE [name] (IF NOT EXISTS); # 创建数据库
DROP DATABASE (IF EXISTS) [name];       # 删除数据库

# 数据表
# 新建表
CREATE TABLE [name] (
		[字段名] [数据类型] NOT NULL UNIQUE,
		[字段名] BOOLEAN DEFAULT TRUE,
		[字段名] [数据类型] PRIMARY KEY AUTO_INCREMENT,
		......
		[字段名] [数据类型],
		PRIMARY KEY([字段名])
);
# 删除表
DROP TABLE (IF EXISTS) [表名];
TRUNCATE TABLE [表名];
# 修改表结构
ALTER TABLE [表名] RENAME TO [新表名];        # 重命名
ALTER TABLE [表名] ADD [列名] [数据类型];      # 新增列
ALTER TABLE [表名] DROP [列名];               # 删除列
ALTER TABLE [表名] MODIFY [列名] [新数据类型]; # 修改数据类型
ALTER TABLE [表名] CHANGE [列名] [新列名] [新数据类型]; # 修改列名和数据类型
```

### DML

```sql
# 增
INSERT INTO [表名] ([列名], [列名], ...) 
VALUES ([值], [值], ...),
			 ...
			 ([值], [值], ...);
			 
# 删
DELETE FROM [表名] [WHERE 条件];
                                                                                                                                                                           
# 改
UPDATE [表名]
SET [列名] = [值], [列名] = [值], ...
[WHERE 条件];

# 查
SELECT [字段列表]          # 4 
FROM [表名]                # 1
WHERE [条件]               # 2
GROUP BY [字段列表]        # 3
HAVING [分组后条件]        
ORDER BY [字段] [ASC/DESC], [字段] [ASC/DESC], ...   # 5
LIMIT [起始索引从0开始], [查询条数]                   # 6
```

### 其他SQL

```sql
# 判断字段是否为null用is null/is not null
where is null

# 删除重复行用distinct关键字
select distinct name from ...

# 字符串函数
select length(str) from ...
substring(列名, start, length)
upper(expr) 大写
lower(expr) 小写
concat(string1, string2, ...)

# 数值函数
# 保留小数位数
select round(avg(a-b), 3) from ...
# 取余数mod(a,b)比a%b效率高
mod(a, b)
# 取整
floor(a)
cell(a)

# count()用法高级
# count()筛选字段条件时要加上or null
count(action='confirmed' or null)
# 统计某列中不重复的记录个数
count(distinct 字段名)

# ifnull类似于getOrDefault()
ifnull(a, 0)

# case语句
case
when ... then ...
when ... then ...
else ...
end

# group_concat()
GROUP_CONCAT(DISTINCT product ORDER BY product SEPARATOR ',')
```

### 日期操作

```sql
# 日期格式化
SELECT DATE_FORMAT('2022-01-08 22:23:01', '%Y%m%d%H%i%s');# 20220108222301
说明：
%W 星期名字(Sunday……Saturday) 
%D 有英语前缀的月份的日期(1st, 2nd, 3rd, 等等。)
%Y 年, 数字, 4 位 ★★★
%y 年, 数字, 2 位
%a 缩写的星期名字(Sun……Sat)
%d 月份中的天数, 数字(00……31) ★★★
%e 月份中的天数, 数字(0……31)
%m 月, 数字(01……12) ★★★ month
%c 月, 数字(1……12)
%b 缩写的月份名字(Jan……Dec)
%j 一年中的天数(001……366)
%H 小时(00……23)★★★
%k 小时(0……23)
%h 小时(01……12) 
%I 小时(01……12)
%l 小时(1……12)
%i 分钟, 数字(00……59) ★★★ minite
%r 时间,12 小时(hh:mm:ss [AP]M)
%T 时间,24 小时(hh:mm:ss)
%S 秒(00……59)
%s 秒(00……59) ★★★
%p AM或PM
%w 一个星期中的天数(0=Sunday ……6=Saturday )
%U 星期(0……52), 这里星期天是星期的第一天,查询指定日期属于当前年份的第几个周 ★★★★
%u 星期(0……52), 这里星期一是星期的第一天

# 日期间隔
# 间隔单位可以是DAY MONTH MINUTE WEEK YEAR SECOND HOUR
SELECT DATE_ADD(NOW(),INTERVAL 2 DAY); # 2022-01-07 22:46:39
SELECT DATE_ADD(NOW(),INTERVAL 2 MONTH); # 2022-02-06 22:47:17
SELECT DATE_ADD('2022-02-06 22:47:17',INTERVAL 2 MONTH); # 2022-04-06 22:47:17
SELECT DATE_SUB(NOW(),INTERVAL 3 DAY); # 2022-01-03 22:49:24
SELECT DATE_SUB('2022-02-06 22:47:17',INTERVAL 2 MONTH); # 2021-12-06 22:47:17

# 日期相差
select datediff('2022-01-06','2021-12-28'); -- 9
```

### 窗口函数

```java
# 窗口函数
聚合函数 over(partition by 列名 order by 列名 rows between 开始 and 结束)
当前行 current row
往前n行 n preceding
往后n行 n following
分区第一行 unbounded preceding
分区最后一行 unbounded following

# 时间范围
聚合函数 over(partition by 列名 order by 列名 range interval 1 day)
```

### 正则表达式

```java
字段 regexp 表达式
```

- `^`：表示一个字符串或行的开头
- `[a-z]`：表示一个字符范围，匹配从 a 到 z 的任何字符。
    - `[0-9]`：表示一个字符范围，匹配从 0 到 9 的任何字符。
    - `[a-zA-Z]`：这个变量匹配从 a 到 z 或 A 到 Z 的任何字符。请注意，你可以在方括号内指定的字符范围的数量没有限制，您可以添加想要匹配的其他字符或范围。
    - `[^a-z]`：这个变量匹配不在 a 到 z 范围内的任何字符。请注意，字符 ^ 用来否定字符范围，它在方括号内的含义与它的方括号外表示开始的含义不同。
- `[a-z]*`：表示一个字符范围，匹配从 a 到 z 的任何字符 0 次或多次。
- `[a-z]+`：表示一个字符范围，匹配从 a 到 z 的任何字符 1 次或多次。
- `.`：匹配任意一个字符。
- `\.`：表示句点字符。请注意，反斜杠用于转义句点字符，因为句点字符在正则表达式中具有特殊含义。还要注意，在许多语言中，你需要转义反斜杠本身，因此需要使用\\.。
- `$`：表示一个字符串或行的结尾。

## 关系的范式

- 1NF：列的原子性，每列都是不可再分的数据单元；
- 2NF：消除部分依赖，除了主键以外的其他列都完全依赖于主键；
- 3NF：消除传递依赖，其他列都与主键直接相关，相互间没有相互依赖关系。

## 数据类型

### char和varchar的区别？

- char存储固定长度的字符，最大长度是255，不足的用空格填充；varchar存储变长的字符，最大字节数是65535，其中需要1-2个字节记录本身长度，允许为null的varchar还需要1个字节标识。
- char存取快，但可能会占据多余的空间；varchar不占据多余空间但是存取慢。

### 记录货币用什么字段类型？

用`Decimal`或者`Numeric`，DECIMAL 和 NUMERIC 值作为字符串存储，而不是作为二进制浮点数，以便保存那些值的小数精度。

### Mysql怎么存储emoji？

MySQL 的 utf8 字符集仅支持最多 3 个字节的 UTF-8 字符，但是 emoji 表情（😊）是 4 个字节的 UTF-8 字符，所以在 MySQL 中存储 emoji 表情时，需要使用 utf8mb4 字符集。

## 执行计划

- 使用`explain`查看。
- 常见字段：
    - `type`：索引
        - `const`：根据主键查询
        - `all`：全表扫描
        - `index`：按索引顺序全表扫描
        - `range`：按索引范围查询
        - `ref`：非唯一索引返回多行数据
        - `eq_ref`：唯一索引返回一行数据
    - `possible_keys`：可能使用的索引
    - `key`：实际使用的索引
    - `key_len`：索引字段长度
    - `extra`：执行情况的描述和说明
        - `using index`：使用覆盖索引
        - `using index condition`：使用索引下推（数据引擎层过滤数据）
        - `using where`：通过where条件过滤数据（server层过滤数据）
        - `using temporary`：查询时，用临时表保存结果，效率低
        - `using filesort`：包含order by操作且无法使用索引排序，效率低

# SQL优化

## 优化策略

1. **合理使用数据库分表**：将冷字段、大字段垂直分表，提高热点数据的查询效率，避免IO争抢和锁表几率。
2. **索引优化**：在常用的字段上建立索引，但要避免过多索引。[索引优化方法](MySQL%2063748c5139f3404594dd27795d6cada0.md) 
3. **避免使用`select *`**：
    1. 增加预处理器的处理成本；
    2. 无用字段增加网络时延和磁盘IO？；
    3. 几乎无法触发覆盖索引。
4. **使用合适的数据类型**：
    1. `varchar`长度分配真正的空间；
    2. 少使用`null`，很难查询优化且占额外的索引空间；
    3. 使用整形、枚举替换字符型等。

## 慢查询

数据库查询的执行时间超过指定的超时时间（默认10秒）时，称为慢查询。

原因：

- 查询语句复杂
- 查询数据量大
- 缺少索引
- 硬件资源不足

优化方法：

- 定位慢sql：开启慢查询，通过日志定位慢sql
- 分析：用`explain`分析具体的sql语句
- 判断索引是否失效、选择更好的索引、使用更优化的查询语句

> 首先，开启慢查询日志，设置阈值，通过日志定位慢sql。然后进行优化，如果表数据量比较大，优先考虑索引优化，提升查询的效率，比如覆盖索引、前缀索引、对排序的分组的字段添加索引等。如果还是比较慢，就得分析执行的情况，可以用explain关键字来分析，主要看type字段看索引是否失效等情况。除此之外，还可以考虑其他的方面比如连接时是否小表驱动大表、是不是分页导致的性能问题等。最后，除了sql上优化，还可以通过数据缓存、分库分表来改善。
> 

开启慢查询日志：

```
show variables like '%slow_query_log%'
set global slow_query_log=1;
```

# 索引

## 索引概念

- 索引是一种能提高数据库查询效率的数据结构。
- 索引一般存储在磁盘的文件中，它是占用物理空间的。

## 索引类型

### 按数据结构

- B+树索引：只在叶子节点存储数据，且叶子节点采用双链表，适合基于范围的查找。
- Hash索引：无序，插入和等值查询效率高，范围查询效率低。
1. **InnoDB为什么使用B+树？**
    1. 二叉查找树：不平衡
    2. 平衡二叉查找树（AVL）：在插入删除节点的时候效率低
    3. 红黑树：不擅长在磁盘设备中的情况，因为树太高了，增删改查需要的磁盘I/O也多，会成为性能瓶颈。
    4. B+树：
        1. 相对于B树，仅在叶子节点存储数据，非叶子节点只放索引，每个磁盘块能存放更多索引，使B+树矮胖，I/O更少。
        2. B+树有大量冗余的节点（所有非叶子节点都是冗余节点），在插入和删除的时候不会发生树型的变化，效率更高。
        3. 叶子节点采用双链表连接，有利于范围查询。

### 按物理存储

- 聚簇索引（主键索引）：叶子节点存放实际的完整数据，一般为主键。
- 非聚簇索引（二级索引）：叶子节点存放索引列和主键值。
1. **回表**
    
    通过二级索引查找数据，获取主键再从主键索引树种获取数据的过程。
    
2. **覆盖索引**
    
    通过二级索引就能够查询到数据，不需要回表。
    

### 按照逻辑维度

- 主键索引：不允许null
- 普通索引
- 唯一索引
- 单列索引
- 联合索引
1. **索引下推**
    
    将条件判断由server层下推到数据引擎层，根据联合索引中的字段进行判断，过滤掉不满足条件的记录，减少回表操作。
    
2. **最左匹配原则**
    - 查询条件中的列需要从联合索引最左边的列开始，并且不能跳过中间的列。
    - 遇到范围查询时停止匹配，范围查询之后的字段无法使用索引。

## 索引优化策略

### 索引的优缺点

1. 优点：
    - 提高查询效率
2. 缺点：
    - 索引占用物理空间
    - 索引的创建和维护需要消耗时间，降低sql的执行效率

### 索引应用场景

1. 适合创建索引的场景：
    1. 经常查询的或有唯一性要求的列
    2. 常用于排序或分组的表（group by/order by）
    3. 数据量大的表
        
        超大表如何添加索引，添加索引会加锁？
        
        1. 先创建一张跟原表`A`数据结构相同的新表`B`。
        2. 在新表`B`添加需要加上的新索引。
        3. 把原表`A`数据导到新表`B`
        4. `rename`新表`B`为原表的表名`A`，原表`A`换别的表名；
2. 不适合创建索引的场景：
    1. 数据量少的表
    2. 很少查询或更新比较频繁的字段
    3. 区分度低的字段（如性别，只有两种取值）

### 索引失效的场景

1. 使用左模糊匹配或左右模糊匹配
2. 违背了最左匹配原则
3. 在索引列中使用计算、函数、类型转换操作
4. where条件中or前面是索引，后面不是索引
5. mysql 估计使用全表扫描要比使用索引快，则不使用索引
6. 索引字段上使用`is null、is not null`，可能导致索引失效

### 索引优化方法

1. 前缀索引优化：减少索引项的大小
2. 覆盖索引优化：联合索引避免回表操作，减少IO
3. 主键索引最好自增：插入新纪录不会造成页分裂
4. 避免过多的索引

# 事务与锁

## 事务的四大特性ACID以及实现方式？

- 原子性Atomicity
    - 所有的操作要么全部完成，要么都不完成，不存在中间状态。
    - 通过Undo log实现
- 一致性Consistency
    - 事务前后数据满足完整性约束，数据库保持一致状态。
    - 通过其他三个特性保证
- 隔离性Isolation
    - 多个事务并发执行，事务之间不能有干扰。
    - 通过MVCC和锁实现
- 持久性Durability
    - 事务一旦被提交则结果永远保存在数据库中，系统故障也不会丢失
    - 通过redo log实现

## 并行事务会造成哪些问题？

- 脏读：读到其他事务没有提交的数据，对方可能回滚
- 不可重复读：同一个事务中，前后两次读取的数据不一致
- 幻读：同一个事务中，前后读取的数据记录数量不一致

## 四种隔离级别？

- 串行化：加读写锁。
- 可重复读：事务执行中的数据与启动时一致，InnoDB默认的隔离级别。
- 读提交：事务提交后，变更被其他事务看到。
- 读未提交：事务没提交变更就被看到。

![4e98ea2e60923b969790898565b4d643.webp](MySQL%2063748c5139f3404594dd27795d6cada0/4e98ea2e60923b969790898565b4d643.webp)

## 锁的类型有哪些？

### 全局锁

对数据库加锁，一般用于全库逻辑备份。

### 表级锁

- 表锁：`lock tables … read/write` 。
- 元数据锁：不显式使用，防止进行CRUD时，其他线程对表结构进行改变。
- 意向锁：快速判断表里是否有记录被加锁，否则一行一行检查效率低。
- Auto-inc锁：保证字段连续自增。

### 行级锁

- Read Lock：记录锁
- Gap Lock：间隙锁，间隙锁S锁和X锁没有区分，没有互斥关系，会产生死锁问题
- Next-key Lock：临键锁
- 插入意向锁：插入时有Gap Lock阻塞，会产生插入意向锁

## 连问串烧

### 为什么会出现幻读？

行锁只能锁住存在的记录，在读取数据前后有事务插入了数据。

### 幻读会带来什么问题？

破环了数据的一致性和锁的语义（锁没全锁住）。

### 怎么避免幻读？

InnoDB在RR级别通过引入间隙锁避免幻读。

- 针对快照读，MVCC在执行第一个查询语句时会生成Read View，后续的查询都利用该Read View，不会出现幻读问题。
- 针对当前读，由于会拉取最新的数据，会出现幻读。InnoDB引入间隙锁，在执行语句时增加临键锁，其他事务不能在查询的数据范围内增加数据。尽管仍然存在幻读的特殊情况，能很大程度上避免幻读。为了保证高并发情况下出错，建议在开启事务后立即`select … for update`锁住记录。

### 间隙锁为什么会发生死锁？

间隙锁没有S锁和X锁之分，锁之间没有互斥。如果两个事务先分别对间隙加锁，再依次在间隙插入数据就会造成死锁。

- S锁：共享锁，锁定的资源可以被用户读取但不能修改。
- X锁：独占锁，锁定的资源不能被读或修改。

### 怎么解决死锁？

1. 设置事务等待锁的超时时间，超时则回滚；
2. 开启数据库主动死锁检查，发现死锁后其中一个事务自动回滚。

### InnoDB怎么实现读提交？

同样通过MVCC（多版本并发控制）实现，MVCC由Read View和Undo log实现：

- 读提交在每次读取数据时都生成Read View；
- 可重复读在事务启动时创建Read View，并且在事务期间使用同一个Read View（对于快照读）。

### MVCC工作原理？

Read View记录了4个字段：事务id、活跃的事务id列表、活跃的最小事务id以及下一个事务的id。查看数据时，比较undo log记录中的隐藏字段，如果记录不可见顺着指针往之前的版本回溯。

- 如果修改记录的事务id比最小活跃事务id小，则该行记录可见；
- 如果修改记录的事务id比下一个事务id大，则该行记录不可见；
- 如果当前记录的事务id在活跃列表中，则可见。

![RECTIFY_IMG_20240605_145545.jpg](MySQL%2063748c5139f3404594dd27795d6cada0/RECTIFY_IMG_20240605_145545.jpg)

### 为什么RR能实现可重复读而RC不行？

1. 快照读：RR在事务中使用同一个RV，而RC在每次读数据前会更新RV；
2. 当前读：RR通过next-key lock加锁，RC没有gap lock，所以不行。

### 什么是脏页？

内存数据页和磁盘数据页内容不一致，称内存页为“脏页”。InnoDB通过AWL技术等待合适时间将脏页写入磁盘。

### 悲观锁和乐观锁的原理和使用场景？

1. 乐观锁认为对同一数据的并发操作不会总发生，属于小概率事件，不用每次都对数据上锁，也就是不采用数据库自身的锁机制，而是通过程序来实现。比如采用版本号机制或时间戳机制。
2. 悲观锁对数据被其他数据修改持保守态度，会通过数据库自身的锁机制来实现，从而保证数据操作的排它性。

使用场景：

1. 乐观锁：读多写少场景，能提高并发性能；
2. 悲观锁：写操作多的场景，防止事务之间相互影响。

# 日志

## MySQL有哪些日志？

- undo log：在数据引擎层生成。在DB更新时记录，主要用于事务回滚和MVCC。
- redo log：在数据引擎层生成。物理日志，记录对数据页的修改，主要用于崩溃恢复。
- bin log：在server层生成。记录了所有修改数据库状态的 SQL 语句，以及每个语句的执行时间，如 INSERT、UPDATE、DELETE 等，主要用于主从复制等。

### undo log和redo log的区别？

- undo log记录数据更新前的状态，在事务提交前崩溃时恢复。
- redo log记录数据更新后的状态，在事务提交后崩溃时恢复。

### redo log和bin log的区别？

- 适用对象
    - redo log在数据引擎层生成，InnoDB可用；
    - bin log在server层生成，任何数据引擎均可用。
- 文件格式
    - redo log是物理日志
    - bin log有三种格式，包括：
        - statement，记录逻辑操作，有动态函数问题（如处理当前时间）；
        - row，记录数据最终被修改的样子，文件大；
        - mixed，自动选择以上两种格式。
- 写入方式
    - redo log是循环写，日志空间大小固定，刷盘的数据会被擦除；
    - bin log是追加写，写满文件会创建新文件。
- 用途
    - redo log用于掉电恢复等场景；
    - bin log用于主从复制、备份恢复等场景。

### 有了redo log为什么还要bin log？

1. binlog 不知道数据库究竟是在哪一时刻丢失了哪部分数据，只能从备份点开始对 binlog 记录重放来恢复数据，比较耗时。
2. binlog 是服务器层面的功能，redo log 是 innoDB 的功能。redo log 帮助 InnoDB 实现了性能提升、自动恢复。但其他存储引擎是无法使用 redo log 的能力的。

## 刷盘时机

### redo log的刷盘时机？

- MySQL关闭的时候；
- 写入量大于redo log buffer一半的时候；
- 根据设置`innodb_flush_log_at_trx_commit` 。
    
    ![innodb_flush_log_at_trx_commit2.drawio.webp](MySQL%2063748c5139f3404594dd27795d6cada0/innodb_flush_log_at_trx_commit2.drawio.webp)
    

### bin log 的刷盘时机？

![binlogcache.drawio.webp](MySQL%2063748c5139f3404594dd27795d6cada0/binlogcache.drawio.webp)

MySQL提供一个`sync_binlog` 参数来控制数据库的 binlog 刷到磁盘上的频率：

- sync_binlog = 0 的时候，表示每次提交事务都只 write，不 fsync，后续交由操作系统决定何时将数据持久化到磁盘；
- sync_binlog = 1 的时候，表示每次提交事务都会 write，然后马上执行 fsync；
- sync_binlog =N(N>1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。

## 两阶段提交

### 为什么需要两阶段提交？

redo log影响主数据库，bin log影响从数据库，如果一个保存完了，另一个没有保存完系统就崩溃了，会导致主数据库和子数据库（或备份数据）数据不一致。

### 执行一条Update语句会发生什么？

1. 查询与修改数据，生成日志：
    1. 数据引擎先查出数据，如果不在buffer pool中就从磁盘中读入，否则直接将数据返回；
    2. 执行器认为需要对数据进行更新，调用数据引擎接口写入新数据；
    3. InnoDB生成undo log（旧数据），修改buffer pool中的undo页，并将对undo页的修改以redo log的方式记录（写入log buffer中的redo log buffer）。
    4. 更新数据（修改buffer pool中的数据页）后记录redo log。
        
        > 无论是undo页的数据还是数据页的数据，都会由后台决定什么时候写入磁盘，即WAL技术。
        > 
    5. 执行器生成这个操作的bin log，写入执行线程的binlog cache中。
2. 事务提交后，执行**两阶段提交**：
    1. prepare阶段：将XA内部事务ID写到redo log，将事务状态设为prepare，持久化到硬盘；
    2. commit阶段：将XA事务ID写到bin log，然后持久化到磁盘，再将redo log状态设为commit，这时候只需要写到page cache里就行了。

![RECTIFY_IMG_20240609_152445.jpg](MySQL%2063748c5139f3404594dd27795d6cada0/RECTIFY_IMG_20240609_152445.jpg)

# 执行引擎

常见的数据引擎为`InnoDB`和`MyISAM` 。

- 事务支持：`InnoDB`支持事务，具有ACID特性；`MyISAM`不支持事务。
- 锁：`InnoDB`支持行级锁；`MyISAM`使用表级锁。
- 外键约束：`InnoDB`支持外键约束；`MyISAM`不支持。
- 崩溃恢复：`MyISAM` 不支持数据库异常崩溃后的恢复，`InnoDB`通过`redo log`保证。
- 索引实现：`MyISAM` 索引的叶子节点保存完整的数据记录，`InnoDB` 索引文件和数据文件分离。

使用场景：

- `InnoDB`：适用于需要事务支持、并发性能好、有高写入需求的应用；
- `MyISAM`：适用于读操作频繁、写操作少的应用。

# 主从复制

## 用途

1. 数据实时备份
2. 读写分离：写请求锁表时不影响读请求（主写从读）
3. 架构拓展，分担负载

## 原理

![mysql-1bfbfcb5-2392-4f98-be1b-a66204da09e5.png](MySQL%2063748c5139f3404594dd27795d6cada0/mysql-1bfbfcb5-2392-4f98-be1b-a66204da09e5.png)

1. 从节点创建I/O线程连接主节点。
2. 主节点创建log dump线程，将指定文件和位置后的日志发送给子节点。
3. 从节点I/O线程获取到数据写入relay log，并保存文件和位置方便下次请求。
4. 从节点SQL线程读relay log回放。

## 主从复制模型

1. 同步复制：主库提交事务的线程等待从库复制成功再响应客户端；
2. 异步复制：主线程不等待bin log同步到从库直接返回；
3. 半同步复制：有部分子库响应成功就返回。

# 分库分表

## 基础理论

- 什么是分库分表？
    
    将原来独立的数据库分为若干个数据库，将独立的大表分为若干小表。
    
- 为什么要分库分表？
    
    解决由于库表数据量过大而导致数据库性能降低的问题。
    
    其他方式：
    
    - 软优化：
        - 分析慢查询sql语句，分析执行计划，进行sql改写；
        - 优化数据库表结构；
        - 优化索引等。
    - 提升硬件配置-成本与性能提升不匹配。
- 什么时候分库分表？
    - 不要先回答分库分表！非必须不对数据库进行分库分表，首先采取其他方式。
    - 单表行数超过`500万`行或者单表容量超过`2GB`，才推荐进行分库分表。
- 如何选择分表键？
    
    找到业务的主题，在业务中经常查询使用的字段作为分片键。
    
- 分表策略有哪些？
    1. 范围分表。例如，按照时间范围划分，不同的年月订单放到不同的表：
        - 优点： 有利于扩容；
        - 缺点：可能会有热点问题。例如查询时间最新数据的请求多。
    2. 哈希取模。取哈希值再取余：
        - 优点：不会存在明显的热点问题；
        - 缺点：前期规划不好，扩容麻烦，解决方式：一致性哈希
    
    3. 范围+哈希
    

## 分库分表的方式

### 垂直分表

按照指定字段拆分表，每张表存储一部分数据，拆解了原有的表结构：

- 优点：充分提高了**热点数**据的操作效率，商品信息的操作的高效率不会被商品描述的低效率所拖累（冷热数据分离）；避免IO争抢和锁表几率。
- 原则：将大字段、不常用的字段拆分，将经常组合查询的列放在一张表中。

### 垂直分库

垂直分表提高了热点数据的查询效率，但没有突破单台服务器的性能瓶颈。垂直分库将**不同类型的表**分布到不同的数据库上，每个库可以放在不同的服务器上。

- 优点：改善了单机硬件资源瓶颈；对不同业务的数据进行分别管理，便于维护。
- 原则：专库专用，公共表等量部署避免联查。

### 水平分表

水平分表就是在同一个数据库内，把同一个表的数据按一定规则拆到多个表中，表的结构没有变化。

- 好处：解决单表数据过大产生的性能问题；避免IO争抢并减少锁表的几率。

### 水平分库

水平分表仅仅解决了单表数据量过大的问题，但是没有解决单库数据量过大的问题。水平分库把同一个表的数据按一定规则拆到不同的数据库中，每个库又可以部署到不同的服务器上。

- 好处：解决单库大数据，高并发的性能瓶颈。

## 问题

1. 事务的一致性：分布式事务问题，不同的数据库甚至服务器完成一个事务。
2. 跨节点关联查询：关联查询的表不在一个数据库→业务开发时需要将原关联查询变成两次查询。
3. 跨节点分页、排序：在每个分片上执行相应的函数，再对总体的结果执行相同的函数。如果数据量大性能很差。
4. 主键避重：主键重复。

## 中间件ShardingSphere

Sharding-JDBC 是ShardingSphere种的一个模块，是一个轻量级 Java 框架，在 JDBC 层提供了扩展性服务。

### 用Sharding-JDBC实现分库分表

1. 导入依赖；
2. 配置数据源，一个数据源相当于一个库，配置每个数据源的连接、用户名、密码等；
3. 配置广播表（广播表属于数据库中数据量较小和变动较少，且存在高频联合查询的表）；
    
    ```python
    # 指定t_dict为公共表，多个公共表以逗号间隔
    spring.shardingsphere.sharding.broadcast‐tables=t_dict
    ```
    
4. 设置默认数据源，没有分片处理的访问默认数据源；
    
    ```python
    # 配置默认数据源（特点：对于不做分片处理的操作，都会直接访问默认数据源
    # 未配置分片规则的表将通过默认数据源定位
    spring.shardingsphere.sharding.default-data-source-name=ds4
    ```
    
5. 垂直分库或水平分库分表需要配置，配置真实库表与逻辑库表的映射、分片键和分片算法
    1. inline模式
        
        ```python
        # 由数据源名 + 表名组成，以小数点分隔。多个表以逗号分隔，支持 inline 表达式。
        spring.shardingsphere.sharding.tables.t_order.actual-data-nodes=ds$->{1..2}.t_order_$->{1..2}
        
        # 定义库分片
        spring.shardingsphere.sharding.tables.t_order.database-strategy.inline.sharding-column=user_id
        # 分片算法行表达式，需符合 groovy 语法
        spring.shardingsphere.sharding.tables.t_order.database-strategy.inline.algorithm-expression=ds$->{user_id % 2 +1}
        ```
        
    2. 标准模式
        
        ```python
        # 用于单分片键的标准分片场景
        sharding.jdbc.config.sharding.tables.<logic-table-name>.database-strategy.standard.sharding-column= # 分片列名称
        sharding.jdbc.config.sharding.tables.<logic-table-name>.database-strategy.standard.precise-algorithm-class-name= # 精确分片算法类名称，用于 = 和 IN。该类需实现 PreciseShardingAlgorithm 接口并提供无参数的构造器
        sharding.jdbc.config.sharding.tables.<logic-table-name>.database-strategy.standard.range-algorithm-class-name= # 范围分片算法类名称，用于 BETWEEN，可选。该类需实现 RangeShardingAlgorithm 接口并提供无参数的构造器
        ```
        
        实现Precise/RangeShardingAlgorithm接口，重写doSharding方法，返回数据源。传入数据源（表）集合、分片信息（分片键、分片值、逻辑表名称）
        
        ```java
        public class CommonAlgorithm4Db implements PreciseShardingAlgorithm<Long>, RangeShardingAlgorithm<Long> {
        
            /**
             * 定义匹配数据源的方法
             * @param dsNames 所有配置的数据源的集合ds${1..2} 包含ds1 ds2封装到该集合下
             *                说白了就是sharinding把所有的数据源都给你，然后让你根据片键值选择
             * @param shardingValue 封装分片相关信息
             * @return 返回匹配的数据源
             */
            @Override
            public String doSharding(Collection<String> dsNames, PreciseShardingValue<Long> shardingValue) {
                //获取数据库分片的字段名称 user_id = in
                String columnName = shardingValue.getColumnName();
                //获取逻辑表名称
                String logicTableName = shardingValue.getLogicTableName();
                //获取片键对应的值 select * from t_order where user_id=10,这里的value就等于10
                Long value = shardingValue.getValue();
                //一般是根据片键值获取对应的数据源，并返回
                String sufix=String.valueOf(value % 2 +1);
                String dsName = dsNames.stream().filter(ds -> ds.endsWith(sufix)).findFirst().get();
                return dsName;
            }
        
            /**
             *
             * @param dsNames 数据源集合
             * @param shardingValue 范围查询信息封装
             * @return
             */
            @Override
            public Collection<String> doSharding(Collection<String> dsNames, RangeShardingValue<Long> shardingValue) {
                //获取分片字段
                String columnName = shardingValue.getColumnName();
                //获取逻辑表
                String logicTableName = shardingValue.getLogicTableName();
                //获取范围数据
                Range<Long> valueRange = shardingValue.getValueRange();
                //select * from t_order where user_id between 1 and 20;
                //判断是否有上限值
                if (valueRange.hasUpperBound()) {
                    //获取上限值 20
                    Long uppper = valueRange.upperEndpoint();
                    System.out.println(uppper);
                }
                if (valueRange.hasLowerBound()){
                    Long lower = valueRange.lowerEndpoint();
                    System.out.println(lower);
                }
                //理论上要根据上限值和下限值获取满足条件的数据源集合
                return Arrays.asList("ds1","ds2");
            }
        }
        ```
        

### 原理流程

1. 解析SQL，获取分片键值，如order_id；
2. 通过配置的分片键和分片算法改写SQL语句；
3. 执行改写后的真实SQL语句；
4. 将所有真正执行SQL的结果进行汇总合并并返回。